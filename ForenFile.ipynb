{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Technologies for Forensic Profiling\n",
    "\n",
    "> This project aims to explore the speech and machine (deep) learning related technologies for forensic profiling.\n",
    "\n",
    "## Motivations\n",
    "**Speech forensics** employs speech processing technologies to discover rich information contained in (concealed) speech associated with suspects, and provides evidence that could be used in court. \n",
    "\n",
    "These information includes\n",
    "1. identity-related\n",
    "    \n",
    "    name, gender, age, height, weight, race, language & dialect, facial & body characteristics\n",
    "2. geographical-related\n",
    "    \n",
    "    speech occurance location & conditions, trace map\n",
    "3. social-relation-related\n",
    "    \n",
    "    home, family members, educatoin, work, party, social status, connections, upbringing\n",
    "4. personal-traits-related\n",
    "    \n",
    "    mental state, personality, emotion tendency, habits\n",
    "5. health-related\n",
    "\n",
    "    illness history, disease tendency, DNA, body shape, composition and size of their vocal tract, skeletal proportions, lung volume and breathing functions\n",
    "6. criminal-records-related\n",
    "    \n",
    "    crime history, crime tendency\n",
    "\n",
    "**Justification**\n",
    "\n",
    "We know that the acoustical aspects of speech are closely related to the speaker's articulatory system, which is further related to the speaker's facial structure and movement, and even to many other physical characteristics. The recordings also contains environmental information that can be exploited. Furthermore, the semantic aspects of speech contains a lot useful information.\n",
    "\n",
    "The sub-objectives are\n",
    "1. Discover disguised voice\n",
    "\n",
    "    How to tell whether a speech is disguised or not? \n",
    "2. Discover voice under manipulatoin\n",
    "\n",
    "    How can we tell if a speaker is under pressure, or threatened, etc.?\n",
    "3. Privacy\n",
    "\n",
    "    How can we pretect the privacy of speakers while preserving their confidential information?\n",
    "4. Reconstruction\n",
    "    \n",
    "    Can we 3D reconstruct the speaker?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "A disguised voice can be a person deliberately impersonates another person, or machine synthesized. In the disguise, the time-frequency traits and semantic traits are altered. Either way, in order to break the disguise, we need to identify the *invariant* and *variant* characteristics in speech. Some factors are innate while some can be modified. We first identify these factors (as mentioned in last part). We then need to define the *normal* manner of a speaker talking, and categorize and quantitize the *deviation* of speech. Using discovered factors, we create forensic profile of the speaker. With this profile, we are able to do a varity of prediction / classification tasks.\n",
    "\n",
    "1. Microstructures: the sub-phonetic level features\n",
    "    \n",
    "    Voice onset time\n",
    "    \n",
    "    harmonic bandwidth\n",
    "\n",
    "    Creak / Vocal fry\n",
    "    \n",
    "    Excitation\n",
    "\n",
    "    Modulation\n",
    "\n",
    "    Formant frequencies\n",
    "\n",
    "    Formant bandwidth\n",
    "\n",
    "    Formant dispersion\n",
    "\n",
    "    Glottal airflow / Glottal pulse shape\n",
    "\n",
    "    Harmonicity / Peak-to-valley ratio\n",
    "\n",
    "    Long-term average spectra\n",
    "\n",
    "    Nasality\n",
    "\n",
    "    Pitch\n",
    "\n",
    "    Register\n",
    "\n",
    "    Resonance\n",
    "\n",
    "    Voice bar \n",
    "\n",
    "    Voice Bar bandwidth\n",
    "\n",
    "    Voice coil peak displacement\n",
    "    \n",
    "    gradient\n",
    "    \n",
    "    region of interest\n",
    "    \n",
    "    neural nets auto-discovered features\n",
    "    \n",
    "2. Hypothesis & tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update 3/5 - 3/11\n",
    "    1. Microfeatures\n",
    "        a. For tidigits, construct speaker-feature dictionary. Speaker dict contains id, gender, age, dialect, seq info. Feature dict contains speaker id, spectrograms, mel spectrograms, const-q spectrograms, mfccs, etc. Seperate training and test set. Seperate single digits and seqs.        \n",
    "        b. For timit, segment by word and by phone. Compute spectrograms and mfccs.\n",
    "        c. Write an interface for general tasks: input speaker id and retrieve its features; input features and predict speaker-ids.\n",
    "        d. Try a Conv-deconv network for feature extraction.\n",
    "    2. Interspeech\n",
    "        a. Compute metrics.\n",
    "        b. Make figures.\n",
    "    2. Qual preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Update 5/8 - 5/13\n",
    "\n",
    "### 1. Get familiar with datasets\n",
    "> TIMIT\n",
    "    * total 6300 sentences, 10 sentences spoken by each of 630 speakers\n",
    "    * 8 major dialect regions of the United States\n",
    "    \n",
    "    \n",
    "    1. **Dialect distribution**\n",
    "    \n",
    "    Table 1:  Dialect distribution of speakers\n",
    "    \n",
    "Dialect Region(dr) | # Male | # Female | Total\n",
    ":------------------|-------:|---------:|------:\n",
    "1 New England       |31 (63%) |18 (27%)  | 49 (8%)  \n",
    "2 Northern       |71 (70%) |31 (30%)  |102 (16%) \n",
    "3 North Midland       |79 (67%) |23 (23%)  |102 (16%) \n",
    "4 South Midland       |69 (69%) |31 (31%)  |100 (16%) \n",
    "5 Southern       |62 (63%) |36 (37%)  | 98 (16%) \n",
    "6 New York City       |30 (65%) |16 (35%)  | 46 (7%) \n",
    "7 Western       |74 (74%) |26 (26%)  |100 (16%) \n",
    "8 Army Brat (moved around)       |22 (67%) |11 (33%)  | 33 (5%)\n",
    "total      |438 (70%)|192 (30%) |630 (100%)\n",
    "    \n",
    "    2. **Corpus text**\n",
    "    \n",
    "    Table 2:  TIMIT speech material\n",
    "\n",
    "Sentence Type |  #Sentences |  #Speakers |  Total |  #Sentences/Speaker\n",
    ":------------- | ---------- :| ---------:| -----:| ------------------:\n",
    "Dialect (SA)|         2|         630|       1260|           2\n",
    "Compact (SX)|        450|           7|       3150|           5\n",
    "Diverse (SI)|       1890|           1|       1890|           3\n",
    "Total|              2342|            |       6300|          10\n",
    "\n",
    "    3. **Filesystem**\n",
    "    \n",
    "    /<CORPUS>/<USAGE>/<DIALECT>/<SEX><SPEAKER_ID>/<SENTENCE_ID>.<FILE_TYPE>\n",
    "         SPEAKER_ID :== <INITIALS><DIGIT>\n",
    "             INITIALS :== speaker initials, 3 letters\n",
    "             DIGIT :== number 0-9 to differentiate speakers with identical initials\n",
    "                             \n",
    "    .wav - waveform file (SPHERE-headered)\n",
    "    .txt - transcription\n",
    "    .wrd - time-aligned word transcription\n",
    "    .phn - time-aligned phonetic transcription\n",
    "    \n",
    "    Examples:\n",
    "     /timit/train/dr1/fcjf0/sa1.wav\n",
    "                         \n",
    "     (TIMIT corpus, training set, dialect region 1, female speaker, \n",
    "      speaker-ID \"cjf0\", sentence text \"sa1\", speech waveform file)\n",
    "      \n",
    "    4. **Other docs**\n",
    "        \n",
    "        sentences, dict, lexicon, alignment\n",
    "        \n",
    "        tagging\n",
    "        \n",
    "    5. **Extra info**\n",
    "    \n",
    "        Birthday, height, race, education\n",
    "        \n",
    "        low pitch, concious attemp to change accent, denasality, inhale/exhale, slow rate, high freq, intonation \n",
    "        \n",
    "        /R/ in \"WASH\", whistling /S/'S, \n",
    "        \n",
    "        movement\n",
    "        \n",
    "        hearing loss, cold, glottal fry, hoarse, voice disorder\n",
    "        \n",
    "        mixed-race, multi-lingual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TIDIGITS\n",
    "    * more than 25 thousand digit sequences\n",
    "    * 326 speakers (111 men, 114 women, 50 boys, and 51 girls)\n",
    "    * collected in a quiet environment and digitized at 20 kHz\n",
    "    \n",
    "    1. **Speaker statistics**\n",
    "        1. Age distribution\n",
    "        2. Dialect distribution\n",
    "        \n",
    "    2. **Corpus text**\n",
    "    \n",
    "    3. **Filesystem**\n",
    "        \n",
    "        FILESPEC ::= /tidigits/<USAGE>/<SPEAKER-TYPE>/<SPEAKER-ID>/<DIGIT-STRING><PRODUCTION>.wav\n",
    "             USAGE ::= test | train\n",
    "             SPEAKER-ID ::= aa | ab | ac | ... | tc\n",
    "             \n",
    "        Example:\n",
    "         /tidigits/train/man/fd/6z97za.wav\n",
    "\n",
    "         (\"tidigits\" corpus, training material, adult male, speaker code \"fd\", digit sequence \"six zero nine seven \n",
    "         zero\", first production, NIST SPHERE file.)\n",
    "         \n",
    "    4. **Other docs**\n",
    "    \n",
    "    5. **Extra info**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Listen, visualize, and compare\n",
    "    \n",
    "    1. Listen samples from the two datasets\n",
    "    \n",
    "    2. Compute their spectra, visulize and analyze\n",
    "    \n",
    "    3. Discoveries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Get familiar with Sphinx\n",
    "\n",
    "    1. Read docs\n",
    "    \n",
    "    2. Read codes and run demos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Other\n",
    "    \n",
    "    1. Code maintenance: format data io, extract and visualize features, previous speech align and segmentation codes, general interfaces\n",
    "    \n",
    "    2. Read relevant papers: speech production, deep kernel learning\n",
    "    \n",
    "    3. Summarize statistical learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
