{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voice based Human Profiling and Forensics\n",
    "\n",
    "> This project aims to explore the methodologies and technologies for voice based human profiling and voice based forensics.\n",
    "\n",
    "> Keywords: voice, biometrics, profiling (describe people), forensics (technologies to assist crime detection), machine learning, signal processing\n",
    "\n",
    "## Motivations\n",
    "**Speech forensics** employs speech processing technologies to discover rich information contained in (concealed) speech associated with suspects, and provides evidence that could be used in court. \n",
    "\n",
    "These information includes\n",
    "1. identity-related\n",
    "    \n",
    "    name, gender, age, height, weight, race, language & dialect, facial & body characteristics\n",
    "2. geographical-related\n",
    "    \n",
    "    speech occurance location & conditions, trace map\n",
    "3. social-relation-related\n",
    "    \n",
    "    home, family members, educatoin, work, party, social status, connections, upbringing\n",
    "4. personal-traits-related\n",
    "    \n",
    "    mental state, personality, emotion tendency, habits\n",
    "5. health-related\n",
    "\n",
    "    illness history, disease tendency, DNA, body shape, composition and size of their vocal tract, skeletal proportions, lung volume and breathing functions\n",
    "6. criminal-records-related\n",
    "    \n",
    "    crime history, crime tendency\n",
    "\n",
    "**Justification**\n",
    "\n",
    "We know that the acoustical aspects of speech are closely related to the speaker's articulatory system, which is further related to the speaker's facial structure and movement, and even to many other physical characteristics. The recordings also contains environmental information that can be exploited. Furthermore, the semantic aspects of speech contains a lot useful information.\n",
    "\n",
    "The sub-objectives are\n",
    "1. Discover disguised voice\n",
    "\n",
    "    How to tell whether a speech is disguised or not? \n",
    "2. Discover voice under manipulatoin\n",
    "\n",
    "    How can we tell if a speaker is under pressure, or threatened, etc.?\n",
    "3. Privacy\n",
    "\n",
    "    How can we pretect the privacy of speakers while preserving their confidential information?\n",
    "4. Reconstruction\n",
    "    \n",
    "    Can we 3D reconstruct the speaker?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "### Break voice disguise\n",
    "Model a person’s normal speech state;  tell if a speech is natural or manipulated/controlled.\n",
    "### Voice profiling\n",
    "Build connections between speech models and a person’s profiles (physical, physiological, psychological factors, etc.).\n",
    "### Voice hologram\n",
    "Reconstruct a 3D figure of a person (and the surroundings) with the profiles from the speech; then build a hologram from the 3D figure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "A disguised voice can be a person deliberately impersonates another person, or machine synthesized. In the disguise, the time-frequency traits and semantic traits are altered. Either way, in order to break the disguise, we need to identify the *invariant* and *variant* characteristics in speech. Some factors are innate while some can be modified. We first identify these factors (as mentioned in last part). We then need to define the *normal* manner of a speaker talking, and categorize and quantitize the *deviation* of speech. Using discovered factors, we create forensic profile of the speaker. With this profile, we are able to do a varity of prediction / classification tasks.\n",
    "\n",
    "1. Microstructures: the sub-phonetic level features\n",
    "    \n",
    "    Voice onset time\n",
    "    \n",
    "    harmonic bandwidth\n",
    "\n",
    "    Creak / Vocal fry\n",
    "    \n",
    "    Excitation\n",
    "\n",
    "    Modulation\n",
    "\n",
    "    Formant frequencies\n",
    "\n",
    "    Formant bandwidth\n",
    "\n",
    "    Formant dispersion\n",
    "\n",
    "    Glottal airflow / Glottal pulse shape\n",
    "\n",
    "    Harmonicity / Peak-to-valley ratio\n",
    "\n",
    "    Long-term average spectra\n",
    "\n",
    "    Nasality\n",
    "\n",
    "    Pitch\n",
    "\n",
    "    Register\n",
    "\n",
    "    Resonance\n",
    "\n",
    "    Voice bar \n",
    "\n",
    "    Voice Bar bandwidth\n",
    "\n",
    "    Voice coil peak displacement\n",
    "    \n",
    "    gradient\n",
    "    \n",
    "    region of interest\n",
    "    \n",
    "    neural nets auto-discovered features\n",
    "    \n",
    "2. Hypothesis & tests\n",
    "\n",
    "3. More\n",
    "\n",
    "![methods](./docs/pics/methods.png)\n",
    "\n",
    "### Normal speech modeling\n",
    "#### Features\n",
    "High: phones, idiolect, semantics, accent, pronunciation, etc.\n",
    "\n",
    "Middle: pitch, energy, duration, rhythm, timbre, etc.\n",
    "\n",
    "Low: temporal-frequency, glottal, etc.\n",
    "\n",
    "Micro: temporal (voice onset time, intra-transition time, gap duration, multi-scale temporal representation), \n",
    "frequential (formant number, position, width, proportions among formants), magnitudial (intensity transitions), statistics (pattern repeat frequency, close-pattern replacement, disappearing patterns), unvoiced pattern, etc.\n",
    "\n",
    "### Quantify & Modeling methods\n",
    "Regression models, spectral models, gradient-based models, statistical models, template-based models\n",
    "Neural net based methods (CNN, RNN, MLP, AE)\n",
    "\n",
    "Graphical models (Bayesian nets, Markov random fields)\n",
    "\n",
    "Neural networks + graphical models (e.g., CNN, LSTM, MLP + HMM, Gaussian process, Markov random fields)\n",
    "\n",
    "### Connect speech models and speaker profiles\n",
    "Classification, regression or generative tasks\n",
    "\n",
    "### Speaker profile reconstruction\n",
    "Generative models\n",
    "\n",
    "For surroundings reconstruction, may need non-speech sounds and active probing methods (e.g., ultrasound arrays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update 3/5 - 3/11\n",
    "    1. Microfeatures\n",
    "        a. For tidigits, construct speaker-feature dictionary. Speaker dict contains id, gender, age, dialect, seq info. Feature dict contains speaker id, spectrograms, mel spectrograms, const-q spectrograms, mfccs, etc. Seperate training and test set. Seperate single digits and seqs.        \n",
    "        b. For timit, segment by word and by phone. Compute spectrograms and mfccs.\n",
    "        c. Write an interface for general tasks: input speaker id and retrieve its features; input features and predict speaker-ids.\n",
    "        d. Try a Conv-deconv network for feature extraction.\n",
    "    2. Interspeech\n",
    "        a. Compute metrics.\n",
    "        b. Make figures.\n",
    "    2. Qual preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Update 5/8 - 5/13\n",
    "\n",
    "### 1. Get familiar with datasets\n",
    "> **TIMIT**\n",
    "    * total 6300 sentences, 10 sentences spoken by each of 630 speakers\n",
    "    * 8 major dialect regions of the United States\n",
    "    \n",
    "    \n",
    "    1. **Dialect distribution**\n",
    "    \n",
    "    Table 1:  Dialect distribution of speakers\n",
    "    \n",
    "Dialect Region(dr) | # Male | # Female | Total\n",
    ":------------------|-------:|---------:|------:\n",
    "1 New England       |31 (63%) |18 (27%)  | 49 (8%)  \n",
    "2 Northern       |71 (70%) |31 (30%)  |102 (16%) \n",
    "3 North Midland       |79 (67%) |23 (23%)  |102 (16%) \n",
    "4 South Midland       |69 (69%) |31 (31%)  |100 (16%) \n",
    "5 Southern       |62 (63%) |36 (37%)  | 98 (16%) \n",
    "6 New York City       |30 (65%) |16 (35%)  | 46 (7%) \n",
    "7 Western       |74 (74%) |26 (26%)  |100 (16%) \n",
    "8 Army Brat (moved around)       |22 (67%) |11 (33%)  | 33 (5%)\n",
    "total      |438 (70%)|192 (30%) |630 (100%)\n",
    "    \n",
    "    2. **Corpus text**\n",
    "    The dialect sentences (SA) were meant to expose the dialectal variants of the speakers and were read by all 630 speakers.\n",
    "    \n",
    "    The phonetically-compact sentences (SX) were designed to provide a good coverage of pairs of phones, with extra occurrences of phonetic contexts thought to be either difficult or of particular interest. Each speaker read 5 of these sentences and each text was spoken by 7 different speakers.\n",
    "    \n",
    "    The phonetically-diverse sentences (SI) were selected from existing text sources - the Brown Corpus (Kuchera and Francis, 1967) and the Playwrights Dialog (Hultzen, et al., 1964) - so as to add diversity in sentence types and phonetic contexts. The selection criteria maximized the variety of allophonic contexts found in the texts. Each speaker read 3 of these sentences, with each sentence being read only by a single speaker. \n",
    "    \n",
    "    Table 2:  TIMIT speech material\n",
    "\n",
    "Sentence Type |  #Sentences |  #Speakers |  Total |  #Sentences/Speaker\n",
    ":-------------|---------- :| ---------:| -----:| ------------------:\n",
    "Dialect (SA)|         2|         630|       1260|           2\n",
    "Compact (SX)|        450|           7|       3150|           5\n",
    "Diverse (SI)|       1890|           1|       1890|           3\n",
    "Total|              2342|            |       6300|          10\n",
    "\n",
    "    3. **Filesystem**\n",
    "    \n",
    "    /<CORPUS>/<USAGE>/<DIALECT>/<SEX><SPEAKER_ID>/<SENTENCE_ID>.<FILE_TYPE>\n",
    "         SPEAKER_ID :== <INITIALS><DIGIT>\n",
    "             INITIALS :== speaker initials, 3 letters\n",
    "             DIGIT :== number 0-9 to differentiate speakers with identical initials\n",
    "                             \n",
    "    .wav - waveform file (SPHERE-headered)\n",
    "    .txt - transcription\n",
    "    .wrd - time-aligned word transcription\n",
    "    .phn - time-aligned phonetic transcription\n",
    "    \n",
    "    Examples:\n",
    "     /timit/train/dr1/fcjf0/sa1.wav\n",
    "                         \n",
    "     (TIMIT corpus, training set, dialect region 1, female speaker, \n",
    "      speaker-ID \"cjf0\", sentence text \"sa1\", speech waveform file)\n",
    "      \n",
    "    4. **Dataset division**\n",
    "    (1) Roughly 20 to 30% of the corpus should be used for testing purposes,\n",
    "     leaving the remaining 70 to 80% for training.\n",
    "    (2) No speaker should appear in both the training and testing portions.\n",
    "\n",
    "    (3) All the dialect regions should be represented in both subsets, with \n",
    "     at least 1 male and 1 female speaker from each dialect.\n",
    "\n",
    "    (4) The amount of overlap of text material in the two subsets should be\n",
    "     minimized; if possible no texts should be identical.\n",
    "\n",
    "    (5) All the phonemes should be covered in the test material, preferably\n",
    "     each phoneme should occur multiple times in different contexts.\n",
    "\n",
    "The core test set thus contains 192 different texts ((5 SX + 3 SI sentences) x 24 speakers).  To avoid overlap with the training material the 2 SA sentences have been excluded from the core and complete test sets. **THESE SENTENCES ARE INCLUDED ON THE CD-ROM, BUT SHOULD NOT BE USED FOR TRAINING OR TEST PURPOSES.**\n",
    "\n",
    "Table 1: Speakers in the Core Test Set\n",
    "\n",
    "Dialect |       Male |        Female |   #Texts/Speaker |  #Total Texts\n",
    ":-------: | :---------: | :------: | --------------: | ------------: \n",
    "1 |       DAB0, WBT0 |    ELC0 |        8 |              24 \n",
    "2 |       TAS1, WEW0 |    PAS0 |        8 |              24\n",
    "3 |       JMP0, LNT0 |    PKT0 |        8 |              24\n",
    "4 |       LLL0, TLS0 |    JLM0 |        8 |              24\n",
    "5 |       BPM0, KLT0 |    NLP0 |        8 |              24\n",
    "6 |       CMJ0, JDH0 |    MGD0 |        8 |              24\n",
    "7 |       GRT0, NJM0 |    DHC0 |        8 |              24\n",
    "8 |       JLN0, PAM0 |    MLD0 |        8 |              24\n",
    "Total |         16 |         8 |          |             192\n",
    "\n",
    "The complete test set consists of a total of 1344 sentences, 8 sentences from each of the 168 speakers. In this set there are 120 distinct SX texts and 504 different SI texts. Thus, roughly 27% (624) of the texts have been reserved\n",
    "for the test material.\n",
    "\n",
    "Table 2: Dialect Distribution of Speakers in Complete Test Set\n",
    "     \n",
    "Dialect  |   #Male |  #Female |  Total\n",
    ":-------: | -----: | -------: | -----:\n",
    "1|           7|        4|       11\n",
    "2|          18|        8|       26\n",
    "3|          23|        3|       26\n",
    "4|          16|       16|       32\n",
    "5|          17|       11|       28\n",
    "6|           8|        3|       11\n",
    "7|          15|        8|       23\n",
    "8|           8|        3|       11\n",
    "Total|       112|       56|      168\n",
    "      \n",
    "    5. **Other docs**\n",
    "        \n",
    "        sentences, dict, lexicon, alignment, tagging\n",
    "        \n",
    "    6. **Extra info**\n",
    "    \n",
    "        Birthday, height, race, education\n",
    "        \n",
    "        mixed-race, multi-lingual\n",
    "        \n",
    "        high/low pitch, concious attemp to change accent, denasality, inhale/exhale, slow rate, high freq, intonation \n",
    "        \n",
    "        /R/ in \"WASH\", whistling /S/'S, \n",
    "        \n",
    "        movement\n",
    "        \n",
    "        hearing loss, cold, glottal fry, hoarse, voice disorder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TIDIGITS**\n",
    "    * more than 25 thousand digit sequences\n",
    "    * 326 speakers (111 men, 114 women, 50 boys, and 51 girls)\n",
    "    * collected in a quiet environment and digitized at 20 kHz\n",
    "    \n",
    "    1. **Speaker statistics**\n",
    "        1. Age distribution\n",
    "TABLE 1.  Number and Age Ranges of Speakers\n",
    "\n",
    "Category | Symbol | Number | Age Range (years)\n",
    ":--------|-------:|-------:|-----------------:\n",
    "Man | M | 111 | 21 - 70\n",
    "Woman | W | 114 | 17 - 59\n",
    "Boy | B | 50 | 6 - 14\n",
    "Girl | G | 51 | 8 - 15\n",
    "        \n",
    "        2. Dialect distribution\n",
    "        \n",
    "TABLE 2.  Description of Dialects and Distribution of Speakers\n",
    "\n",
    "City|                      Dialect|             M|    W|    B|    G\n",
    ":---|----------------------------:|-------------:|----:|----:|-----:\n",
    "01 Boston, MA|            Eastern New England|      5|    5|    0|    1\n",
    "02 Richmond, VA|          Virginia Piedmont|        5|    5|    2|    4\n",
    "03 Lubbock, TX|           Southwest|                5|    5|    0|    1\n",
    "04 Los Angeles, CA|       Southern California|      5|    5|    0|    1\n",
    "05 Knoxville, TN|         South Midland|            5|    5|    0|    0\n",
    "06 Rochester, NY|         Central New York|         6|    6|    0|    0\n",
    "07 Denver, CO|            Rocky Mountains|          5|    5|    0|    0\n",
    "08 Milwaukee, WS|         North Central|            5|    5|    2|    0\n",
    "09 Philadelphia, PA|      Delaware Valley|          5|    6|    0|    1\n",
    "10 Kansas City, KS|       Midland|                  5|    5|    4|    1\n",
    "11 Chicago, IL|           North Central|            5|    5|    1|    2\n",
    "12 Charleston, SC|        South Carolina|           5|    5|    1|    0\n",
    "13 New Orleans, LA|       Gulf South|               5|    5|    2|    0\n",
    "14 Dayton, OH|            South Midland|            5|    5|    0|    0\n",
    "15 Atlanta, GA|           Gulf South|               5|    5|    0|    1\n",
    "16 Miami, FL|             Spanish American|         5|    5|    1|    0\n",
    "17 Dallas, TX|            Southwest|                5|    5|   34|   36\n",
    "18 New York, NY|          New York City|            5|    5|    2|    2\n",
    "19 Little Rock, AR|       South Midland|            5|    6|    0|    0\n",
    "20 Portland, OR|          Pacific Northwest|        5|    5|    0|    0\n",
    "21 Pittsburgh, PA|        Upper Ohio Valley|        5|    5|    0|    0\n",
    "22|                       Black|                    5|    6|    1|    1\n",
    "|Total Speakers|         111|  114|   50|   51|    326  \n",
    "        \n",
    "    2. **Corpus text**\n",
    "Each speaker provided 253 digits and 176 digit transitions. The procedure of generating the digits sequence makes the **frequency distribution uniform** over all eleven digits. However, the \"zero\"-\"zero\" and \"oh\"-\"oh\" transitions tend to occur **twice** as frequently as any other transition.\n",
    "\n",
    "**Quite data acquisition**; Utterances were digitized using a Digital Sound Corporation Model 200 **16-bit** A/D/A. The sampling rate was **20 kHz**, and a **10 kHz anti-aliasing** filter was used. \n",
    "\n",
    "TABLE 2.  Corpus text types\n",
    "\n",
    "Type | No. of sentences\n",
    ":----|----------------:\n",
    "isolated digits (two tokens of each of the eleven digits) | 22\n",
    "two-digit sequences | 11\n",
    "three-digit sequences | 11\n",
    "four-digit sequences | 11\n",
    "five-digit sequences | 11\n",
    "seven-digit sequences | 11\n",
    "TOTAL | 77\n",
    "    \n",
    "    3. **Filesystem**\n",
    "        \n",
    "        FILESPEC ::= /tidigits/<USAGE>/<SPEAKER-TYPE>/<SPEAKER-ID>/<DIGIT-STRING><PRODUCTION>.wav\n",
    "             USAGE ::= test | train\n",
    "             SPEAKER-ID ::= aa | ab | ac | ... | tc\n",
    "             \n",
    "        Example:\n",
    "         /tidigits/train/man/fd/6z97za.wav\n",
    "\n",
    "         (\"tidigits\" corpus, training material, adult male, speaker code \"fd\", digit sequence \"six zero nine seven \n",
    "         zero\", first production, NIST SPHERE file.)\n",
    "    \n",
    "The filename assigned to each data file consists of 3 to 9 characters and is of the form \"NSI\".\n",
    "\n",
    "The symbol N represents a string of 1 to 7 of the characters Z,1,2,3,4,5,6,7,8,9,O and indicates the spoken digit sequence.\n",
    "\n",
    "The symbol S represents a 2-letter speaker designator (initials). (The letters Z and O are not used in speaker designators.)\n",
    "\n",
    "The symbol I is either null or a single digit, and is used to distinguish multiple utterances of the same digit sequence by the same speaker. The absence of a digit indicates there is only one utterance of the digit sequence by the speaker, while the presence of a digit M, say, indicates the M-th utterance of the digit sequence by the speaker. \n",
    "\n",
    "For example, the filename \"23Z45MA\" was assigned to the file containing the first or only utterance of the sequence \"2 3 zero 4 5\" by speaker designated \"MA\". The filename \"ODF2\" was assigned to the file containing the second utterance of the sequence \"oh\" by the speaker designated \"DF\".\n",
    "\n",
    "    4. **Dataset division**\n",
    "TABLE 7.  Number of Speakers as a Function of Speaker Category -- Test and Train\n",
    "\n",
    "Speaker Category |   Man |   Woman |  Boy |   Girl\n",
    ":----------------|------:|--------:|-----:|------:\n",
    "Train|          55|     57|   25|     26\n",
    "Test|           56|     57|   25|     25\n",
    "    \n",
    "    5. **Other docs**\n",
    "many convenient records\n",
    "\n",
    "    6. **Extra info**\n",
    "    \n",
    "    The following information was stored in the header of each data file:\n",
    "\n",
    "    Speaker's name;\n",
    "    Two-character speaker designator;\n",
    "    Speaker's age;\n",
    "    Speaker's dialect classification;\n",
    "    Speaker's category (M,W,B,G);\n",
    "    Speaker's subset (Train, Test);\n",
    "    Sequence of digits uttered.\n",
    "\n",
    "    The dataset provides statistics on **Speaker error** and **Listener error**.\n",
    "    \n",
    "    It also provides the inherent recognizability of the data. Each utterance in the database was downsampled to 12.5 kHz, analyzed and synthesized using 14-th order autocorrelation LPC analysis. A 25 ms window length and 10 ms frame period were used with pre-emphasis constant of 0.9375. Pitch tracking was accomplished using a crosscorrelation algorithm with post-processing. Listeners heard only this synthesized speech. The recognizability of the (LPC synthesized) data was measured as 99.99%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Listen, visualize, and compare\n",
    "    \n",
    "    1. Listen samples from the two datasets\n",
    "    \n",
    "    2. Compute their spectra, visulize and analyze\n",
    "    \n",
    "    3. Discoveries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Get familiar with Sphinx\n",
    "\n",
    "    1. Read docs\n",
    "    \n",
    "    2. Read codes and run demos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Other\n",
    "    \n",
    "    1. Code maintenance: format data io, extract and visualize features, previous speech align and segmentation codes, general interfaces\n",
    "    \n",
    "    2. Read relevant papers: speech production, deep kernel learning\n",
    "    \n",
    "    3. Summarize statistical learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Update 5/14 - 5/20\n",
    "\n",
    "### New thoughts on feature extraction and speaker profiling\n",
    "(discussed with Yandong)\n",
    "> Build a framework (classical Guassian models or neural nets) that takes in **speech spectrograms** and outputs **speaker profiles** (id and id related: gender, age, height, race, education, language, dialects). Then **trace back** from outputs to inputs, and find **active regions**/inducing parts in inputs and their **activate path to output**. Next, build a graphical model upon the active regions and the hidden parameters (such as pitch, breathing patterns, speaking patterns, illness, etc).\n",
    "\n",
    "> This idea aims to exploit the structures existed in both the input and the output spaces, and connect the two spaces with neural net models.\n",
    "\n",
    "#### Steps\n",
    "1. Baseline: (1) Build Gaussian models to do speaker recognition; (2) Build neural network models to do speaker recognition; (3) Get decent performance.\n",
    "\n",
    "2. Back tracking: Trace back from output to input to obtain active path and active region.\n",
    "\n",
    "3. Active region modeling: build a graphical model (such as the Ising model or RBM) to model the active region with corresponding hidden parameters. Then we can inference the graph based on observations. By doing so we can find the connection between the **specific nodes in the graph**(the patterns in spectrograms) and the **observed profiles**.\n",
    "\n",
    "4. Using the **discovered patterns in spectrograms** and their indications, we can further build a simplified predictor to do speaker recognition and related tasks.\n",
    "\n",
    "### Others\n",
    "1. Update previous sections in this notebook.\n",
    "\n",
    "2. Building baseline.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Update ~ 05/29/2017\n",
    "\n",
    "### Summary\n",
    "1. wrote my own scripts for force alignment\n",
    "**NOTE:** TO DO ALL TASKS TOGETHER:\n",
    "```bash\n",
    "./scripts/feat_align_extract_join.master.sh\n",
    "```\n",
    "\n",
    "2. Writing code for baselines\n",
    "\n",
    "### Force align audio && extract phones and words\n",
    "#### 0. Extract features: MFCC / logspec\n",
    "    \n",
    "    1. Prepare control files: filelists\n",
    "timit_train_wavlists.ctl\n",
    "![](./docs/pics/timit_train_list.png)\n",
    "\n",
    "```bash\n",
    "# command example\n",
    "find ../timit_data/timit/ -name \"*.wav\" | grep -v \"maxed\" | grep \"train\" | sed \"s/.*timit\\///g\" | sort > timit_train_wavlist.ctl\n",
    "```\n",
    "\n",
    "    2. Run scripts to extract mfcc / logspec\n",
    "```bash\n",
    "# command example\n",
    "# NOTE: All scripts have to be RUN OUT OF scripts/ --> ./scripts/script_name\n",
    "./scripts/compute_feat.sh --type mfcc -f ./lists/timit_train_wavlist.ctl -i ../timit_data/timit -o ./mfcc -t ./tmp\n",
    "\n",
    "# view mfcc\n",
    "./wave2feat/sphinx_cepview -f ./mfcc/train/dr1/fcjf0/sa1.80-7200-40f.mfc\n",
    "```\n",
    "sampled mfcc\n",
    "![](./docs/pics/mfcc_sample.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Force align\n",
    "    1. Prepare transcripts, dicts, phonelists\n",
    "transcripts: trans \\ file pairs ![timit_trans](./docs/pics/timit_trans.png)\n",
    "dicts ![timit_dicts](./docs/pics/timit_dict.png)\n",
    "\n",
    "    2. Decode and align\n",
    "phone segments\n",
    "![](./docs/pics/ph_seg.png)\n",
    "word segments\n",
    "![](./docs/pics/word_seg.png)\n",
    "```bash\n",
    "# command example\n",
    "./scripts/falign_timit.sh --part 1 --npart 1 --listbasedir ./lists/transcripts --ctlf timit_train.ctl --transf timit_correct.trans --dictf timit.dict --fillerf timit.fillerdict --mfcdir ./mfcc --outputdir falign_out --outsegdir phseg_out --jobname timit_falign_05292017 &\n",
    "\n",
    "# NOTE: can check correctness by reading falign_out/*.log\n",
    "```\n",
    "\n",
    "**NOTE:** 1. alignment may differ each time.\n",
    "2. Misalignment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Extract  phones and words\n",
    "    1. Prepare phone ctls\n",
    "Get ctl for each force-aligned phone.\n",
    "![](./docs/pics/phnctl_output.png)\n",
    "![](./docs/pics/phone_ctl.png)\n",
    "```bash\n",
    "# command example\n",
    "./scripts/makectl.sh  -indir ./phseg_out -type phone -phase train -ext phseg -tmpdir tmp -outdir phctl_out\n",
    "```\n",
    "\n",
    "    2. Extract phone segments\n",
    "```bash\n",
    "# command example\n",
    "./scripts/extract_join_segs.sh -ctl phctl_out/AA.ctl -inwav ../timit_data/timit/train -ext wav -outwav phsegwav_out -join true -jpath ./ -jnum 100\n",
    "```\n",
    "    3. Hear and visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra: Using cluster: qsub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Update ~ 06/04/2017\n",
    "### Summary\n",
    "1. Read literature on speaker recognition\n",
    "2. Work on SRE with GMM, i-vector\n",
    "3. Work on SRE with DNN\n",
    "\n",
    "### 1. Literature: speech inversion\n",
    "**Main idea:**\n",
    "1. use DNN/CNN to predict articulatory trajectories from speech\n",
    "    \n",
    "    Data: multi-speaker synthetic articulatory data by varying vocal tract length, pitch, articulatory weights, etc. Generated by using the Haskins Laboratories Task Dynamics Application -- CMU dictionary fed to TADA to generates vocal tract constriction variables and corresponding synthetic speech.\n",
    "    \n",
    "    **Tract-variable trajectories / vocal tract constriction variables (*TVs*) **: variables defining 5 **constrictors** (lip, tongue tip, tongue body, velum, glottis) states, and are results of **articulatory activities**; the variables are formulated into motion equations -- **critically damped second-order systems**.\n",
    "    \n",
    "    Use **Pearson product-moment correlation (PPMC) coefficient** to measure amplitude and dynamic similarity between the groundtruth and the estimated TVs -- they actually are correlation.\n",
    "    \n",
    "2. then use two parallel CNNs on articulatory information and acoustical information (gammatone-filterbanked spectrograms) to do speech recognition\n",
    "\n",
    "**Remarks:** \n",
    "\n",
    "1. This builds up connection between articulatory kinetics and a set of governing variables. Then we can do speech inversion to learn (TVs, acoustical feats (normalized modulation coefficients)). By previous proposition in _Update 5/14 - 5/20_, we do back tracking through NN to find the active region in spectrograms, thus we find (active regions, speech). Now we propose to further relate (TVs, active regions) through possibly joint learning.\n",
    "\n",
    "2. The TVs are discrete models of articulatory activities, while in reality articulatory activities are continuous and overlapped. We propose to form a continuous model instead.\n",
    "\n",
    "ref: https://www.sri.com/work/publications/hybrid-convolutional-neural-networks-articulatory-and-acoustic-information-based\n",
    "\n",
    "**Questions:**\n",
    "1. what are senones?\n",
    "\n",
    "### 2. Speaker recognition with GMM based methods\n",
    "1. UBM-GMM/i-vector model (with source-normalization)\n",
    "\n",
    "*Use tools: Bob*\n",
    "https://pythonhosted.org/bob/index.html#\n",
    "\n",
    "### 3. Speaker recognition with NN based methods\n",
    "0. DNN/i-vector model\n",
    "\n",
    "1. DNN bottle neck model\n",
    "\n",
    "2. CNN model\n",
    "\n",
    "3. RNN model\n",
    "\n",
    "*Implemented with PyTorch*\n",
    "\n",
    "### To Dos\n",
    "1. Continue building baselines\n",
    "\n",
    "2. Study NN backtracking, GAN and other generative models\n",
    "\n",
    "https://128.84.21.199/pdf/1706.00550.pdf\n",
    "\n",
    "3. Work on theories on (articulatory, acoustical, speech and speaker id) modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update ~ 06/25/2017\n",
    "\n",
    "### 0. Summary\n",
    "> (With Yandong) Implement and run speaker recognition baselines.\n",
    "\n",
    "> Paper reading\n",
    "\n",
    "### 1. Baselines\n",
    "1. speaker recognition on timit\n",
    "\n",
    "```bash\n",
    "# Example command\n",
    "./ivector_sr.py --vv -d timit -p energy-2gauss -e mfcc-60 -a ${ALGORITHM} -s timit-${ALGORITHM} -T temp -R results --parallel <N>\n",
    "# Evaluate\n",
    "evaluate.py -d ${PATH_TO_SCORES_DEV} -c EER\n",
    "```\n",
    "\n",
    "baseline | config  | EER (%)\n",
    ":--------|:-------:|-------:\n",
    "mfcc-60 + ubm-gmm | 128\\*gaussians | 5.367\n",
    "mfcc-60 + ivector + cosine | 512\\*gaussians, subspace dim 400 | 4.762\n",
    "mfcc-60 + ivector + plda | 256\\*gaussians, subspace dim 100, G_F dim 50 | running\n",
    "mfcc-60 + ivector + lda-wccn-plda | lda dim 50, 256\\*gaussians, subspace dim 100, G_F dim 50 | running\n",
    "mfcc-60 + jfa | 512\\*gaussians, U_V dim 2 | 11.607\n",
    "dnn | |\n",
    "cnn | 4\\*conv + 1\\*pool + 2\\*fc | Acc 92~94\n",
    "rnn | |\n",
    "\n",
    "**Remarks:**\n",
    "\n",
    "preprocessing: 2 Gaussian energy-based VAD\n",
    "\n",
    "mfcc-60: (19 MFCC features + Energy) + First and second derivatives\n",
    "\n",
    "wccn: within-class covariance normalization\n",
    "\n",
    "### 2. Paper reading\n",
    "\n",
    "* Wavenet: a generative model for raw audio\n",
    "\n",
    "**remarks:**\n",
    "\n",
    "    a. neural generative model, autoregressive, model joint prob with products of conditions, dilated causal convolutions (==time delay nns), gated, residual, skip conn, \n",
    "    b. context stacks?\n",
    "    c. GAN or Dual: (speaker, context dependent) speech synthesis v.s. speech recog\n",
    "    d. Text to speech:\n",
    "![](./docs/pics/tts.png)\n",
    "\n",
    "\n",
    "### Todos\n",
    "\n",
    "1. work on baseline network dissection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update ~ 7/3/2017\n",
    "### 0. Summary\n",
    "> classification tasks with phonemes\n",
    "\n",
    "> DCGAN on phonemes\n",
    "\n",
    "### 1.  Classification with phonemes\n",
    "\n",
    "speaker / dialect / height / education / race classification with phonemes\n",
    "\n",
    "use **spectrograms of phonemes** as input\n",
    "\n",
    "net | input | label | acc (%)\n",
    ":---|------:|------:|-------:\n",
    "cnn | AA | speaker id |\n",
    " | AE | |\n",
    " | AO | |\n",
    " | B | |\n",
    " | D | |\n",
    " | EY | |\n",
    " | G | |\n",
    " | HH | |\n",
    " | IY | |\n",
    " | K | |\n",
    " | L | |\n",
    " | M | |\n",
    " | N | |\n",
    " | NG | |\n",
    " | OW | |\n",
    " | P | |\n",
    " | S | |\n",
    " | UW | |\n",
    " | V | |\n",
    " | W | |\n",
    " | Y | |\n",
    " | Z | |\n",
    "\n",
    "\n",
    "### 2. DCGAN on phonemes\n",
    "\n",
    "use Deep Convolutional GAN to find internal representation of phonemes\n",
    "\n",
    "\n",
    "ref:\n",
    "\n",
    "[1] Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks\n",
    "\n",
    "https://arxiv.org/abs/1511.06434\n",
    "\n",
    "### 3. VAE on phonemes\n",
    "\n",
    "use Variational Autoencoder to find internal representation of phonemes\n",
    "\n",
    "ref:\n",
    "\n",
    "[1] Stochastic Gradient VB and the Variational Auto-Encoder\n",
    "\n",
    "https://arxiv.org/pdf/1312.6114.pdf\n",
    "\n",
    "### Todos\n",
    "\n",
    "1. CNN with Gaussian filters\n",
    "\n",
    "2. Net dissection\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
