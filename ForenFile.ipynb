{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Technologies for Forensic Profiling\n",
    "\n",
    "> This project aims to explore the speech and machine (deep) learning related technologies for forensic profiling.\n",
    "\n",
    "## Motivations\n",
    "**Speech forensics** employs speech processing technologies to discover rich information contained in (concealed) speech associated with suspects, and provides evidence that could be used in court. \n",
    "\n",
    "These information includes\n",
    "1. identity-related\n",
    "    \n",
    "    name, gender, age, height, weight, race, language & dialect, facial & body characteristics\n",
    "2. geographical-related\n",
    "    \n",
    "    speech occurance location & conditions, trace map\n",
    "3. social-relation-related\n",
    "    \n",
    "    home, family members, educatoin, work, party, social status, connections, upbringing\n",
    "4. personal-traits-related\n",
    "    \n",
    "    mental state, personality, emotion tendency, habits\n",
    "5. health-related\n",
    "\n",
    "    illness history, disease tendency, DNA, body shape, composition and size of their vocal tract, skeletal proportions, lung volume and breathing functions\n",
    "6. criminal-records-related\n",
    "    \n",
    "    crime history, crime tendency\n",
    "\n",
    "**Justification**\n",
    "\n",
    "We know that the acoustical aspects of speech are closely related to the speaker's articulatory system, which is further related to the speaker's facial structure and movement, and even to many other physical characteristics. The recordings also contains environmental information that can be exploited. Furthermore, the semantic aspects of speech contains a lot useful information.\n",
    "\n",
    "The sub-objectives are\n",
    "1. Discover disguised voice\n",
    "\n",
    "    How to tell whether a speech is disguised or not? \n",
    "2. Discover voice under manipulatoin\n",
    "\n",
    "    How can we tell if a speaker is under pressure, or threatened, etc.?\n",
    "3. Privacy\n",
    "\n",
    "    How can we pretect the privacy of speakers while preserving their confidential information?\n",
    "4. Reconstruction\n",
    "    \n",
    "    Can we 3D reconstruct the speaker?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "A disguised voice can be a person deliberately impersonates another person, or machine synthesized. In the disguise, the time-frequency traits and semantic traits are altered. Either way, in order to break the disguise, we need to identify the *invariant* and *variant* characteristics in speech. Some factors are innate while some can be modified. We first identify these factors (as mentioned in last part). We then need to define the *normal* manner of a speaker talking, and categorize and quantitize the *deviation* of speech. Using discovered factors, we create forensic profile of the speaker. With this profile, we are able to do a varity of prediction / classification tasks.\n",
    "\n",
    "1. Microstructures: the sub-phonetic level features\n",
    "    \n",
    "    Voice onset time\n",
    "    \n",
    "    harmonic bandwidth\n",
    "\n",
    "    Creak / Vocal fry\n",
    "    \n",
    "    Excitation\n",
    "\n",
    "    Modulation\n",
    "\n",
    "    Formant frequencies\n",
    "\n",
    "    Formant bandwidth\n",
    "\n",
    "    Formant dispersion\n",
    "\n",
    "    Glottal airflow / Glottal pulse shape\n",
    "\n",
    "    Harmonicity / Peak-to-valley ratio\n",
    "\n",
    "    Long-term average spectra\n",
    "\n",
    "    Nasality\n",
    "\n",
    "    Pitch\n",
    "\n",
    "    Register\n",
    "\n",
    "    Resonance\n",
    "\n",
    "    Voice bar \n",
    "\n",
    "    Voice Bar bandwidth\n",
    "\n",
    "    Voice coil peak displacement\n",
    "    \n",
    "    gradient\n",
    "    \n",
    "    region of interest\n",
    "    \n",
    "    neural nets auto-discovered features\n",
    "    \n",
    "2. Hypothesis & tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update 3/5 - 3/11\n",
    "    1. Microfeatures\n",
    "        a. For tidigits, construct speaker-feature dictionary. Speaker dict contains id, gender, age, dialect, seq info. Feature dict contains speaker id, spectrograms, mel spectrograms, const-q spectrograms, mfccs, etc. Seperate training and test set. Seperate single digits and seqs.        \n",
    "        b. For timit, segment by word and by phone. Compute spectrograms and mfccs.\n",
    "        c. Write an interface for general tasks: input speaker id and retrieve its features; input features and predict speaker-ids.\n",
    "        d. Try a Conv-deconv network for feature extraction.\n",
    "    2. Interspeech\n",
    "        a. Compute metrics.\n",
    "        b. Make figures.\n",
    "    2. Qual preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
